{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63204b3d",
   "metadata": {
    "papermill": {
     "duration": 0.004799,
     "end_time": "2025-08-10T22:51:43.841688",
     "exception": false,
     "start_time": "2025-08-10T22:51:43.836889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### References\n",
    "\n",
    "*   [https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876](https://www.kaggle.com/code/abdmental01/jigsaw-mpnet-base-v2-inference-cv-0-876)\n",
    "*   [https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo](https://www.kaggle.com/code/aerdem4/jigsaw-acrc-qwen7b-finetune-logits-processor-zoo)\n",
    "*   [https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/](https://www.guruguru.science/competitions/24/discussions/21027ff1-2074-4e21-a249-b2d4170bd516/)\n",
    "*   https://www.kaggle.com/code/mks2192/jigsaw-llama3-1-8b-instruct-training-one-epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14107ad2",
   "metadata": {
    "papermill": {
     "duration": 0.003636,
     "end_time": "2025-08-10T22:51:43.849333",
     "exception": false,
     "start_time": "2025-08-10T22:51:43.845697",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Qwen2.5 32B GPTQ Int4 Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aff54f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T22:51:43.857633Z",
     "iopub.status.busy": "2025-08-10T22:51:43.857398Z",
     "iopub.status.idle": "2025-08-10T22:51:45.743244Z",
     "shell.execute_reply": "2025-08-10T22:51:45.742435Z"
    },
    "papermill": {
     "duration": 1.891637,
     "end_time": "2025-08-10T22:51:45.744616",
     "exception": false,
     "start_time": "2025-08-10T22:51:43.852979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在代码开头添加\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# 清理GPU内存\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# 或者重启kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83a26a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T22:51:45.753139Z",
     "iopub.status.busy": "2025-08-10T22:51:45.752864Z",
     "iopub.status.idle": "2025-08-10T22:51:45.756374Z",
     "shell.execute_reply": "2025-08-10T22:51:45.755650Z"
    },
    "papermill": {
     "duration": 0.009027,
     "end_time": "2025-08-10T22:51:45.757618",
     "exception": false,
     "start_time": "2025-08-10T22:51:45.748591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# 在代码最开头添加\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a860c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T22:51:45.765348Z",
     "iopub.status.busy": "2025-08-10T22:51:45.765109Z",
     "iopub.status.idle": "2025-08-10T22:51:45.891230Z",
     "shell.execute_reply": "2025-08-10T22:51:45.890455Z"
    },
    "papermill": {
     "duration": 0.131475,
     "end_time": "2025-08-10T22:51:45.892569",
     "exception": false,
     "start_time": "2025-08-10T22:51:45.761094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir -p /tmp/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ca3674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T22:51:45.901380Z",
     "iopub.status.busy": "2025-08-10T22:51:45.901125Z",
     "iopub.status.idle": "2025-08-10T22:51:47.792558Z",
     "shell.execute_reply": "2025-08-10T22:51:47.791950Z"
    },
    "papermill": {
     "duration": 1.897556,
     "end_time": "2025-08-10T22:51:47.793878",
     "exception": false,
     "start_time": "2025-08-10T22:51:45.896322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "在原有代码基础上的最小化改进\n",
    "只需要在现有代码的几个关键位置进行修改\n",
    "\"\"\"\n",
    "\n",
    "# ============== 1. 在文件开头添加这些函数 ==============\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def extract_text_features(text, rule, pos_examples, neg_examples):\n",
    "    \"\"\"提取文本特征用于动态权重计算\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # 基础特征\n",
    "    features['text_length'] = len(text)\n",
    "    features['word_count'] = len(text.split())\n",
    "    features['upper_ratio'] = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n",
    "    features['punct_count'] = len(re.findall(r'[!@#$%^&*(),.?\":{}|<>]', text))\n",
    "    features['caps_words'] = len(re.findall(r'\\b[A-Z]{2,}\\b', text))\n",
    "    \n",
    "    # 简单的相似度特征\n",
    "    try:\n",
    "        tfidf = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "        all_texts = [text, rule] + pos_examples + neg_examples\n",
    "        tfidf_matrix = tfidf.fit_transform(all_texts)\n",
    "        \n",
    "        text_vec = tfidf_matrix[0]\n",
    "        rule_vec = tfidf_matrix[1]\n",
    "        pos_vecs = tfidf_matrix[2:4]\n",
    "        neg_vecs = tfidf_matrix[4:6]\n",
    "        \n",
    "        features['rule_similarity'] = cosine_similarity(text_vec, rule_vec)[0][0]\n",
    "        features['pos_similarity'] = np.mean([cosine_similarity(text_vec, pos_vec)[0][0] for pos_vec in pos_vecs])\n",
    "        features['neg_similarity'] = np.mean([cosine_similarity(text_vec, neg_vec)[0][0] for neg_vec in neg_vecs])\n",
    "    except:\n",
    "        features['rule_similarity'] = 0.5\n",
    "        features['pos_similarity'] = 0.5\n",
    "        features['neg_similarity'] = 0.5\n",
    "    \n",
    "    return features\n",
    "\n",
    "def get_adaptive_weights(features, subreddit):\n",
    "    \"\"\"基于特征和subreddit动态计算权重\"\"\"\n",
    "    # 默认权重\n",
    "    qwen_weight = 0.5\n",
    "    llama_weight = 0.5\n",
    "    \n",
    "    # 基于文本长度调整权重\n",
    "    if features['text_length'] > 500:  # 长文本，Qwen可能更好\n",
    "        qwen_weight += 0.1\n",
    "        llama_weight -= 0.1\n",
    "    elif features['text_length'] < 100:  # 短文本，Llama可能更好\n",
    "        qwen_weight -= 0.1\n",
    "        llama_weight += 0.1\n",
    "    \n",
    "    # 基于大写字母比例调整（可能是愤怒/激动的文本）\n",
    "    if features['upper_ratio'] > 0.3:\n",
    "        qwen_weight += 0.05  # Qwen可能在情绪文本上更敏感\n",
    "        llama_weight -= 0.05\n",
    "    \n",
    "    # 基于相似度调整\n",
    "    similarity_diff = features['pos_similarity'] - features['neg_similarity']\n",
    "    if similarity_diff > 0.2:  # 明显更像违规样本\n",
    "        qwen_weight += 0.1\n",
    "        llama_weight -= 0.1\n",
    "    elif similarity_diff < -0.2:  # 明显更像正常样本\n",
    "        qwen_weight -= 0.1\n",
    "        llama_weight += 0.1\n",
    "    \n",
    "    # 基于subreddit调整（可以根据经验添加）\n",
    "    if subreddit in ['AskReddit', 'politics']:  # 这些可能需要更细致的判断\n",
    "        qwen_weight += 0.05\n",
    "        llama_weight -= 0.05\n",
    "    \n",
    "    # 确保权重在合理范围内\n",
    "    qwen_weight = max(0.1, min(0.9, qwen_weight))\n",
    "    llama_weight = 1 - qwen_weight\n",
    "    \n",
    "    return qwen_weight, llama_weight\n",
    "\n",
    "def two_stage_prediction(text, rule, examples, base_prediction):\n",
    "    \"\"\"简单的两阶段推理\"\"\"\n",
    "    # Stage 1: 快速判断是否可能违规\n",
    "    pos_examples = [examples['positive_example_1'], examples['positive_example_2']]\n",
    "    neg_examples = [examples['negative_example_1'], examples['negative_example_2']]\n",
    "    \n",
    "    features = extract_text_features(text, rule, pos_examples, neg_examples)\n",
    "    \n",
    "    # 如果基础预测很低且特征也不像违规，直接返回低分\n",
    "    if base_prediction < 0.3 and features['pos_similarity'] < features['neg_similarity']:\n",
    "        return base_prediction * 0.8  # 进一步降低\n",
    "    \n",
    "    # 如果基础预测很高且特征也像违规，提升分数\n",
    "    if base_prediction > 0.7 and features['pos_similarity'] > features['neg_similarity']:\n",
    "        return min(0.95, base_prediction * 1.1)  # 适度提升\n",
    "    \n",
    "    # Stage 2: 细化判断（根据特征调整）\n",
    "    adjustment = 0\n",
    "    \n",
    "    # 基于文本特征调整\n",
    "    if features['caps_words'] > 2:  # 很多大写单词，可能是愤怒\n",
    "        adjustment += 0.05\n",
    "    \n",
    "    if features['punct_count'] > features['word_count'] * 0.3:  # 标点很多\n",
    "        adjustment += 0.03\n",
    "    \n",
    "    # 基于相似度调整\n",
    "    similarity_ratio = features['pos_similarity'] / max(features['neg_similarity'], 0.01)\n",
    "    if similarity_ratio > 2:\n",
    "        adjustment += 0.1\n",
    "    elif similarity_ratio < 0.5:\n",
    "        adjustment -= 0.1\n",
    "    \n",
    "    final_prediction = base_prediction + adjustment\n",
    "    return max(0.01, min(0.99, final_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bd0b153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T22:51:47.802985Z",
     "iopub.status.busy": "2025-08-10T22:51:47.802682Z",
     "iopub.status.idle": "2025-08-10T22:51:47.810589Z",
     "shell.execute_reply": "2025-08-10T22:51:47.809788Z"
    },
    "papermill": {
     "duration": 0.01382,
     "end_time": "2025-08-10T22:51:47.811702",
     "exception": false,
     "start_time": "2025-08-10T22:51:47.797882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/src/infer_qwen.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/src/infer_qwen.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "import torch\n",
    "import vllm\n",
    "import numpy as np\n",
    "from vllm.lora.request import LoRARequest\n",
    "import argparse\n",
    "from scipy.special import softmax\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ============== 添加辅助函数 ==============\n",
    "\n",
    "def extract_text_features(text, rule, pos_examples, neg_examples):\n",
    "    \"\"\"提取文本特征用于动态权重计算\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # 基础特征\n",
    "    features['text_length'] = len(text)\n",
    "    features['word_count'] = len(text.split())\n",
    "    features['upper_ratio'] = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n",
    "    features['punct_count'] = len(re.findall(r'[!@#$%^&*(),.?\":{}|<>]', text))\n",
    "    features['caps_words'] = len(re.findall(r'\\b[A-Z]{2,}\\b', text))\n",
    "    \n",
    "    # 简单的相似度特征\n",
    "    try:\n",
    "        tfidf = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "        all_texts = [text, rule] + pos_examples + neg_examples\n",
    "        tfidf_matrix = tfidf.fit_transform(all_texts)\n",
    "        \n",
    "        text_vec = tfidf_matrix[0]\n",
    "        rule_vec = tfidf_matrix[1]\n",
    "        pos_vecs = tfidf_matrix[2:4]\n",
    "        neg_vecs = tfidf_matrix[4:6]\n",
    "        \n",
    "        features['rule_similarity'] = cosine_similarity(text_vec, rule_vec)[0][0]\n",
    "        features['pos_similarity'] = np.mean([cosine_similarity(text_vec, pos_vec)[0][0] for pos_vec in pos_vecs])\n",
    "        features['neg_similarity'] = np.mean([cosine_similarity(text_vec, neg_vec)[0][0] for neg_vec in neg_vecs])\n",
    "    except:\n",
    "        features['rule_similarity'] = 0.5\n",
    "        features['pos_similarity'] = 0.5\n",
    "        features['neg_similarity'] = 0.5\n",
    "    \n",
    "    return features\n",
    "\n",
    "def two_stage_prediction(text, rule, examples, base_prediction):\n",
    "    \"\"\"简单的两阶段推理\"\"\"\n",
    "    # Stage 1: 快速判断是否可能违规\n",
    "    pos_examples = [examples['positive_example_1'], examples['positive_example_2']]\n",
    "    neg_examples = [examples['negative_example_1'], examples['negative_example_2']]\n",
    "    \n",
    "    features = extract_text_features(text, rule, pos_examples, neg_examples)\n",
    "    \n",
    "    # 如果基础预测很低且特征也不像违规，直接返回低分\n",
    "    if base_prediction < 0.3 and features['pos_similarity'] < features['neg_similarity']:\n",
    "        return base_prediction * 0.8  # 进一步降低\n",
    "    \n",
    "    # 如果基础预测很高且特征也像违规，提升分数\n",
    "    if base_prediction > 0.7 and features['pos_similarity'] > features['neg_similarity']:\n",
    "        return min(0.95, base_prediction * 1.1)  # 适度提升\n",
    "    \n",
    "    # Stage 2: 细化判断（根据特征调整）\n",
    "    adjustment = 0\n",
    "    \n",
    "    # 基于文本特征调整\n",
    "    if features['caps_words'] > 2:  # 很多大写单词，可能是愤怒\n",
    "        adjustment += 0.05\n",
    "    \n",
    "    if features['punct_count'] > features['word_count'] * 0.3:  # 标点很多\n",
    "        adjustment += 0.03\n",
    "    \n",
    "    # 基于相似度调整\n",
    "    similarity_ratio = features['pos_similarity'] / max(features['neg_similarity'], 0.01)\n",
    "    if similarity_ratio > 2:\n",
    "        adjustment += 0.1\n",
    "    elif similarity_ratio < 0.5:\n",
    "        adjustment -= 0.1\n",
    "    \n",
    "    final_prediction = base_prediction + adjustment\n",
    "    return max(0.01, min(0.99, final_prediction))\n",
    "\n",
    "# ============== 原始代码开始 ==============\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\")\n",
    "\n",
    "MODEL_NAME = \"/kaggle/input/qwen2-5-32b-instruct-gptq-int4\"\n",
    "LORA_PATH = \"/kaggle/input/jigsaw-exp003-fold0/trained_model\"\n",
    "\n",
    "if __name__=='__main__':\n",
    "    os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "    llm = vllm.LLM(\n",
    "        MODEL_NAME,\n",
    "        # quantization='awq',\n",
    "        quantization='gptq',\n",
    "        tensor_parallel_size=torch.cuda.device_count(),\n",
    "        gpu_memory_utilization=0.95,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=4096,\n",
    "        disable_log_stats=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enable_lora=True,\n",
    "    )\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    SYS_PROMPT = \"\"\"\n",
    "    You are given a comment on reddit. Your task is to classify if it violates the given rule. Only respond Yes/No.\n",
    "    \"\"\"\n",
    "\n",
    "    prompts = []\n",
    "    for i, row in df.iterrows():\n",
    "        text = f\"\"\"\n",
    "    r/{row.subreddit}\n",
    "    Rule: {row.rule}\n",
    "\n",
    "    1) {row.positive_example_1}\n",
    "    Violation: Yes\n",
    "\n",
    "    2) {row.positive_example_2}\n",
    "    Violation: Yes\n",
    "\n",
    "    3) {row.negative_example_1}\n",
    "    Violation: No\n",
    "\n",
    "    4) {row.negative_example_2}\n",
    "    Violation: No\n",
    "\n",
    "    5) {row.body}\n",
    "    \"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        ) + \"Answer:\"\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    df[\"prompt\"] = prompts\n",
    "\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=['Yes','No'])\n",
    "    outputs = llm.generate(\n",
    "        prompts,\n",
    "        vllm.SamplingParams(\n",
    "            skip_special_tokens=True,\n",
    "            max_tokens=1,\n",
    "            logits_processors=[mclp],\n",
    "            logprobs=2,\n",
    "        ),\n",
    "        use_tqdm=True,\n",
    "        lora_request=LoRARequest(\"default\", 1, LORA_PATH)\n",
    "    )\n",
    "    logprobs = [\n",
    "        {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n",
    "        for out in outputs\n",
    "    ]\n",
    "    logit_matrix = pd.DataFrame(logprobs)[['Yes','No']]\n",
    "    df = pd.concat([df, logit_matrix], axis=1)\n",
    "\n",
    "    df[['Yes',\"No\"]] = df[['Yes',\"No\"]].apply(lambda x: softmax(x.values), axis=1, result_type=\"expand\")\n",
    "    df[\"pred\"] = df[\"Yes\"]\n",
    "    \n",
    "    # ============== 添加两阶段优化 ==============\n",
    "    print(\"开始两阶段优化...\")\n",
    "    enhanced_predictions = []\n",
    "    for idx, row in df.iterrows():\n",
    "        examples = {\n",
    "            'positive_example_1': row['positive_example_1'],\n",
    "            'positive_example_2': row['positive_example_2'], \n",
    "            'negative_example_1': row['negative_example_1'],\n",
    "            'negative_example_2': row['negative_example_2']\n",
    "        }\n",
    "        enhanced_pred = two_stage_prediction(row['body'], row['rule'], examples, row['pred'])\n",
    "        enhanced_predictions.append(enhanced_pred)\n",
    "\n",
    "    df['rule_violation'] = enhanced_predictions\n",
    "    print(\"两阶段优化完成！\")\n",
    "    # ============== 优化结束 ==============\n",
    "    \n",
    "    df[['row_id', 'rule_violation']].to_csv(\"submission_qwen.csv\",index=False)\n",
    "    print(pd.read_csv('submission_qwen.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e6fdddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T22:51:47.820366Z",
     "iopub.status.busy": "2025-08-10T22:51:47.819910Z",
     "iopub.status.idle": "2025-08-10T22:58:02.929849Z",
     "shell.execute_reply": "2025-08-10T22:58:02.929050Z"
    },
    "papermill": {
     "duration": 375.115618,
     "end_time": "2025-08-10T22:58:02.931220",
     "exception": false,
     "start_time": "2025-08-10T22:51:47.815602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp\n",
      "2025-08-10 22:52:00.219778: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1754866320.601461      60 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1754866320.710638      60 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 08-10 22:52:15 [__init__.py:235] Automatically detected platform cuda.\r\n",
      "INFO 08-10 22:52:32 [config.py:1604] Using max model len 4096\r\n",
      "WARNING 08-10 22:52:33 [config.py:1084] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "WARNING 08-10 22:52:34 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 08-10 22:52:34 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/kaggle/input/qwen2-5-32b-instruct-gptq-int4', speculative_config=None, tokenizer='/kaggle/input/qwen2-5-32b-instruct-gptq-int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/kaggle/input/qwen2-5-32b-instruct-gptq-int4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \r\n",
      "WARNING 08-10 22:52:34 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\r\n",
      "WARNING 08-10 22:52:34 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "INFO 08-10 22:52:34 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 08-10 22:52:34 [cuda.py:395] Using XFormers backend.\r\n",
      "2025-08-10 22:52:41.414726: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1754866361.436212      90 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1754866361.442719      90 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 08-10 22:52:44 [__init__.py:235] Automatically detected platform cuda.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=90)\u001b[0;0m INFO 08-10 22:52:45 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=90)\u001b[0;0m INFO 08-10 22:52:46 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=90)\u001b[0;0m INFO 08-10 22:52:46 [cuda.py:395] Using XFormers backend.\r\n",
      "[W810 22:52:57.380239185 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W810 22:52:57.734156236 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W810 22:53:07.390890950 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W810 22:53:17.401426094 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 08-10 22:53:17 [__init__.py:1375] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=90)\u001b[0;0m INFO 08-10 22:53:17 [__init__.py:1375] Found nccl from library libnccl.so.2\r\n",
      "INFO 08-10 22:53:17 [pynccl.py:70] vLLM is using nccl==2.26.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=90)\u001b[0;0m INFO 08-10 22:53:17 [pynccl.py:70] vLLM is using nccl==2.26.2\r\n",
      "INFO 08-10 22:53:17 [custom_all_reduce_utils.py:208] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 08-10 22:53:41 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "WARNING 08-10 22:53:41 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=90)\u001b[0;0m INFO 08-10 22:53:41 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=90)\u001b[0;0m WARNING 08-10 22:53:41 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n",
      "INFO 08-10 22:53:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_c91ddd13'), local_subscribe_addr='ipc:///tmp/9d8b7439-b79d-4eb4-bcfe-19e0adf21752', remote_subscribe_addr=None, remote_addr_ipv6=False)\r\n",
      "INFO 08-10 22:53:41 [parallel_state.py:1102] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=90)\u001b[0;0m INFO 08-10 22:53:41 [parallel_state.py:1102] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\r\n",
      "INFO 08-10 22:53:41 [model_runner.py:1083] Starting to load model /kaggle/input/qwen2-5-32b-instruct-gptq-int4...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=90)\u001b[0;0m INFO 08-10 22:53:42 [model_runner.py:1083] Starting to load model /kaggle/input/qwen2-5-32b-instruct-gptq-int4...\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:30<02:00, 30.04s/it]\r\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [01:05<01:39, 33.00s/it]\r\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [01:52<01:19, 39.71s/it]\r\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [02:40<00:42, 42.70s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [03:27<00:00, 44.34s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [03:27<00:00, 41.47s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=90)\u001b[0;0m INFO 08-10 22:57:10 [default_loader.py:262] Loading weights took 207.02 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=90)\u001b[0;0m INFO 08-10 22:57:10 [logger.py:65] Using PunicaWrapperGPU.\r\n",
      "INFO 08-10 22:57:10 [default_loader.py:262] Loading weights took 207.66 seconds\r\n",
      "INFO 08-10 22:57:10 [logger.py:65] Using PunicaWrapperGPU.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=90)\u001b[0;0m INFO 08-10 22:57:11 [model_runner.py:1115] Model loading took 9.2529 GiB and 207.720875 seconds\r\n",
      "INFO 08-10 22:57:11 [model_runner.py:1115] Model loading took 9.2529 GiB and 208.345384 seconds\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=90)\u001b[0;0m INFO 08-10 22:57:30 [worker.py:295] model weights take 9.25GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.61GiB; the rest of the memory reserved for KV Cache is 4.03GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 08-10 22:57:30 [worker.py:295] model weights take 9.25GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 1.43GiB; the rest of the memory reserved for KV Cache is 3.21GiB.\r\n",
      "INFO 08-10 22:57:30 [executor_base.py:113] # cuda blocks: 1645, # CPU blocks: 2048\r\n",
      "INFO 08-10 22:57:30 [executor_base.py:118] Maximum concurrency for 4096 tokens per request: 6.43x\r\n",
      "INFO 08-10 22:57:34 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 23.26 seconds\r\n",
      "Adding requests: 100%|██████████████████████████| 10/10 [00:01<00:00,  8.71it/s]\r\n",
      "Processed prompts: 100%|█| 10/10 [00:18<00:00,  1.84s/it, est. speed input: 189.\r\n",
      "开始两阶段优化...\r\n",
      "两阶段优化完成！\r\n",
      "   row_id  rule_violation\r\n",
      "0    2029        0.411120\r\n",
      "1    2030        0.152700\r\n",
      "2    2031        0.484380\r\n",
      "3    2032        0.496019\r\n",
      "4    2033        0.763484\r\n",
      "5    2034        0.071056\r\n",
      "6    2035        0.693168\r\n",
      "7    2036        0.035215\r\n",
      "8    2037        0.244369\r\n",
      "9    2038        0.563410\r\n",
      "ERROR 08-10 22:57:56 [multiproc_worker_utils.py:121] Worker VllmWorkerProcess pid 90 died, exit code: -15\r\n",
      "INFO 08-10 22:57:56 [multiproc_worker_utils.py:125] Killing local vLLM worker processes\r\n",
      "[rank0]:[W810 22:57:58.074863388 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"
     ]
    }
   ],
   "source": [
    "%cd /tmp\n",
    "!python src/infer_qwen.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3012ef",
   "metadata": {
    "papermill": {
     "duration": 0.005333,
     "end_time": "2025-08-10T22:58:02.942402",
     "exception": false,
     "start_time": "2025-08-10T22:58:02.937069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Llama3.1 8B Instruct Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c2b0ed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T22:58:02.953870Z",
     "iopub.status.busy": "2025-08-10T22:58:02.953636Z",
     "iopub.status.idle": "2025-08-10T22:58:02.957654Z",
     "shell.execute_reply": "2025-08-10T22:58:02.957047Z"
    },
    "papermill": {
     "duration": 0.011073,
     "end_time": "2025-08-10T22:58:02.958700",
     "exception": false,
     "start_time": "2025-08-10T22:58:02.947627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, math, numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a93ff25e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T22:58:02.970671Z",
     "iopub.status.busy": "2025-08-10T22:58:02.970226Z",
     "iopub.status.idle": "2025-08-10T22:58:03.550015Z",
     "shell.execute_reply": "2025-08-10T22:58:03.549322Z"
    },
    "papermill": {
     "duration": 0.586978,
     "end_time": "2025-08-10T22:58:03.551215",
     "exception": false,
     "start_time": "2025-08-10T22:58:02.964237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule_violation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2036</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rule_violation\n",
       "row_id                \n",
       "2029               0.5\n",
       "2030               0.5\n",
       "2031               0.5\n",
       "2032               0.5\n",
       "2033               0.5\n",
       "2034               0.5\n",
       "2035               0.5\n",
       "2036               0.5\n",
       "2037               0.5\n",
       "2038               0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')\n",
    "sub = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/sample_submission.csv', index_col='row_id')\n",
    "sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15e388fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T22:58:03.563903Z",
     "iopub.status.busy": "2025-08-10T22:58:03.563094Z",
     "iopub.status.idle": "2025-08-10T23:01:02.538014Z",
     "shell.execute_reply": "2025-08-10T23:01:02.537110Z"
    },
    "papermill": {
     "duration": 178.982506,
     "end_time": "2025-08-10T23:01:02.539548",
     "exception": false,
     "start_time": "2025-08-10T22:58:03.557042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 22:58:06.416583: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754866686.439721      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754866686.446490      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-10 22:58:12 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 08-10 22:58:28 [config.py:1604] Using max model len 2048\n",
      "WARNING 08-10 22:58:28 [arg_utils.py:1690] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "WARNING 08-10 22:58:28 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 08-10 22:58:28 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/kaggle/input/jigsaw-llama3-1-8b-instruct-training-one-epoch/llama-8b-instruct-jigsaw', speculative_config=None, tokenizer='/kaggle/input/jigsaw-llama3-1-8b-instruct-training-one-epoch/llama-8b-instruct-jigsaw', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/kaggle/input/jigsaw-llama3-1-8b-instruct-training-one-epoch/llama-8b-instruct-jigsaw, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "WARNING 08-10 22:58:29 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "WARNING 08-10 22:58:29 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 08-10 22:58:31 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-10 22:58:31 [cuda.py:395] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-10 22:58:34.196643: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754866714.217237     398 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754866714.223826     398 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-10 22:58:39 [__init__.py:235] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=398)\u001b[0;0m INFO 08-10 22:58:40 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=398)\u001b[0;0m INFO 08-10 22:58:41 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=398)\u001b[0;0m INFO 08-10 22:58:41 [cuda.py:395] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W810 22:58:52.512202172 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W810 22:58:52.883159025 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W810 22:59:02.522867645 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-10 22:59:12 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=398)\u001b[0;0m INFO 08-10 22:59:12 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=398)\u001b[0;0m INFO 08-10 22:59:12 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 08-10 22:59:12 [pynccl.py:70] vLLM is using nccl==2.26.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W810 22:59:12.533412463 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-10 22:59:12 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=398)\u001b[0;0m INFO 08-10 22:59:12 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=398)\u001b[0;0m WARNING 08-10 22:59:12 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 08-10 22:59:12 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 08-10 22:59:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_e6a94384'), local_subscribe_addr='ipc:///tmp/92e100b3-d156-447e-b99c-f3bc3f6d4ef3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 08-10 22:59:12 [parallel_state.py:1102] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorkerProcess pid=398)\u001b[0;0m INFO 08-10 22:59:12 [parallel_state.py:1102] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 08-10 22:59:12 [model_runner.py:1083] Starting to load model /kaggle/input/jigsaw-llama3-1-8b-instruct-training-one-epoch/llama-8b-instruct-jigsaw...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=398)\u001b[0;0m INFO 08-10 22:59:12 [model_runner.py:1083] Starting to load model /kaggle/input/jigsaw-llama3-1-8b-instruct-training-one-epoch/llama-8b-instruct-jigsaw...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476adb90cbc14fdaba4b5f2cd02eff4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=398)\u001b[0;0m INFO 08-10 23:00:52 [default_loader.py:262] Loading weights took 99.46 seconds\n",
      "INFO 08-10 23:00:52 [default_loader.py:262] Loading weights took 99.55 seconds\n",
      "INFO 08-10 23:00:53 [model_runner.py:1115] Model loading took 7.5123 GiB and 99.785241 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=398)\u001b[0;0m INFO 08-10 23:00:53 [model_runner.py:1115] Model loading took 7.5123 GiB and 99.701905 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=398)\u001b[0;0m INFO 08-10 23:00:57 [worker.py:295] Memory profiling takes 3.71 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=398)\u001b[0;0m INFO 08-10 23:00:57 [worker.py:295] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.95) = 14.00GiB\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=398)\u001b[0;0m INFO 08-10 23:00:57 [worker.py:295] model weights take 7.51GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.14GiB; the rest of the memory reserved for KV Cache is 6.25GiB.\n",
      "INFO 08-10 23:00:58 [worker.py:295] Memory profiling takes 3.82 seconds\r\n",
      "INFO 08-10 23:00:58 [worker.py:295] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.95) = 14.00GiB\r\n",
      "INFO 08-10 23:00:58 [worker.py:295] model weights take 7.51GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 1.22GiB; the rest of the memory reserved for KV Cache is 5.17GiB.\n",
      "INFO 08-10 23:00:58 [executor_base.py:113] # cuda blocks: 5289, # CPU blocks: 4096\n",
      "INFO 08-10 23:00:58 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 41.32x\n",
      "INFO 08-10 23:01:02 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 8.72 seconds\n"
     ]
    }
   ],
   "source": [
    "import vllm\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    \"/kaggle/input/jigsaw-llama3-1-8b-instruct-training-one-epoch/llama-8b-instruct-jigsaw\",\n",
    "    tensor_parallel_size=2, \n",
    "    gpu_memory_utilization=0.95, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=2048,\n",
    "    # disable_log_stats=True,\n",
    "    # enable_prefix_caching=True,\n",
    "    \n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4daeddb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T23:01:02.556424Z",
     "iopub.status.busy": "2025-08-10T23:01:02.555759Z",
     "iopub.status.idle": "2025-08-10T23:01:02.566158Z",
     "shell.execute_reply": "2025-08-10T23:01:02.565299Z"
    },
    "papermill": {
     "duration": 0.019766,
     "end_time": "2025-08-10T23:01:02.567429",
     "exception": false,
     "start_time": "2025-08-10T23:01:02.547663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Force predictions to be tokens [2822, 9642] which are ['No', 'Yes'].\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, List\n",
    "from transformers import LogitsProcessor\n",
    "import torch\n",
    "\n",
    "choices = [\"No\", \"Yes\"]\n",
    "\n",
    "KEEP = []\n",
    "for x in choices:\n",
    "    c = tokenizer.encode(x,add_special_tokens=False)[0]\n",
    "    KEEP.append(c)\n",
    "print(f\"Force predictions to be tokens {KEEP} which are {choices}.\")\n",
    "\n",
    "class DigitLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.allowed_ids = KEEP\n",
    "        \n",
    "    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:\n",
    "        scores[self.allowed_ids] += 100\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7b8bf06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T23:01:02.582794Z",
     "iopub.status.busy": "2025-08-10T23:01:02.582569Z",
     "iopub.status.idle": "2025-08-10T23:01:02.586080Z",
     "shell.execute_reply": "2025-08-10T23:01:02.585464Z"
    },
    "papermill": {
     "duration": 0.012307,
     "end_time": "2025-08-10T23:01:02.587109",
     "exception": false,
     "start_time": "2025-08-10T23:01:02.574802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sys_prompt = '''You are given a comment on reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6473cfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T23:01:02.602933Z",
     "iopub.status.busy": "2025-08-10T23:01:02.602475Z",
     "iopub.status.idle": "2025-08-10T23:01:02.606579Z",
     "shell.execute_reply": "2025-08-10T23:01:02.605776Z"
    },
    "papermill": {
     "duration": 0.01329,
     "end_time": "2025-08-10T23:01:02.607829",
     "exception": false,
     "start_time": "2025-08-10T23:01:02.594539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def formatting(dataset):\n",
    "    texts = []\n",
    "    for i in range(len(dataset)):\n",
    "        texts.append(tokenizer.apply_chat_template(dataset[i], tokenize=False, add_generation_prompt=False))\n",
    "    return texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9ffa8e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T23:01:02.624221Z",
     "iopub.status.busy": "2025-08-10T23:01:02.623864Z",
     "iopub.status.idle": "2025-08-10T23:01:02.627217Z",
     "shell.execute_reply": "2025-08-10T23:01:02.626449Z"
    },
    "papermill": {
     "duration": 0.012303,
     "end_time": "2025-08-10T23:01:02.628616",
     "exception": false,
     "start_time": "2025-08-10T23:01:02.616313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "template = \"\"\"\n",
    "Subreddit: r/{subreddit}\n",
    "Rule: {rule}\n",
    "Examples:\n",
    "1) {positive_example_1}\n",
    "Violation: Yes\n",
    "\n",
    "2) {negative_example_1}\n",
    "Violation: No\n",
    "\n",
    "3) {negative_example_2}\n",
    "Violation: No\n",
    "\n",
    "4) {positive_example_2}\n",
    "Violation: Yes\n",
    "Comment:\n",
    "{body}\n",
    "Violation: \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "568a9777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T23:01:02.656209Z",
     "iopub.status.busy": "2025-08-10T23:01:02.656031Z",
     "iopub.status.idle": "2025-08-10T23:01:02.663659Z",
     "shell.execute_reply": "2025-08-10T23:01:02.662913Z"
    },
    "papermill": {
     "duration": 0.021747,
     "end_time": "2025-08-10T23:01:02.664893",
     "exception": false,
     "start_time": "2025-08-10T23:01:02.643146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for index,row in test.iterrows():\n",
    "    \n",
    "    formatted_sample = [\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": sys_prompt\n",
    "    },\n",
    "       {\n",
    "           \"role\": \"user\",\n",
    "           \"content\": template.format(\n",
    "               rule = row.rule,\n",
    "               subreddit = row.subreddit,\n",
    "               body = row.body,\n",
    "               positive_example_1 = row.positive_example_1,\n",
    "               negative_example_1 = row.negative_example_1,\n",
    "               positive_example_2 = row.positive_example_2,\n",
    "               negative_example_2 = row.negative_example_2\n",
    "           )\n",
    "       }]\n",
    "    \n",
    "    dataset.append( formatted_sample )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc98f60f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T23:01:02.682315Z",
     "iopub.status.busy": "2025-08-10T23:01:02.682079Z",
     "iopub.status.idle": "2025-08-10T23:01:02.704689Z",
     "shell.execute_reply": "2025-08-10T23:01:02.703975Z"
    },
    "papermill": {
     "duration": 0.031909,
     "end_time": "2025-08-10T23:01:02.705922",
     "exception": false,
     "start_time": "2025-08-10T23:01:02.674013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_prompts = formatting(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edeebf44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T23:01:02.723131Z",
     "iopub.status.busy": "2025-08-10T23:01:02.722647Z",
     "iopub.status.idle": "2025-08-10T23:01:04.429930Z",
     "shell.execute_reply": "2025-08-10T23:01:04.429398Z"
    },
    "papermill": {
     "duration": 1.716589,
     "end_time": "2025-08-10T23:01:04.431202",
     "exception": false,
     "start_time": "2025-08-10T23:01:02.714613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4a4ef7afb847ca91a3a1f966373582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70893d08054541888154da85d6378fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logits_processors = [DigitLogitsProcessor(tokenizer)]\n",
    "responses = llm.generate(\n",
    "    all_prompts,\n",
    "    vllm.SamplingParams(\n",
    "        n=1,  # Number of output sequences to return for each prompt.\n",
    "        top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "        temperature=0,  # randomness of the sampling\n",
    "        seed=777, # Seed for reprodicibility\n",
    "        skip_special_tokens=True,  # Whether to skip special tokens in the output.\n",
    "        max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "        logits_processors=logits_processors,\n",
    "        logprobs = 2\n",
    "    ),\n",
    "    use_tqdm = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86d01a95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T23:01:04.448584Z",
     "iopub.status.busy": "2025-08-10T23:01:04.447920Z",
     "iopub.status.idle": "2025-08-10T23:01:04.457558Z",
     "shell.execute_reply": "2025-08-10T23:01:04.456356Z"
    },
    "papermill": {
     "duration": 0.018908,
     "end_time": "2025-08-10T23:01:04.458786",
     "exception": false,
     "start_time": "2025-08-10T23:01:04.439878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 0 inference errors out of 10 inferences\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "errors = 0\n",
    "\n",
    "for i,response in enumerate(responses):\n",
    "    try:\n",
    "        x = response.outputs[0].logprobs[0]\n",
    "        logprobs = []\n",
    "        for k in KEEP:\n",
    "            if k in x:\n",
    "                logprobs.append( math.exp(x[k].logprob) )\n",
    "            else:\n",
    "                logprobs.append( 0 )\n",
    "                print(f\"bad logits {i}\")\n",
    "        logprobs = np.array( logprobs )\n",
    "        logprobs /= logprobs.sum()\n",
    "        results.append( logprobs )\n",
    "    except:\n",
    "        #print(f\"error {i}\")\n",
    "        results.append( np.array([1/2., 1/2.]) )\n",
    "        errors += 1\n",
    "        \n",
    "print(f\"There were {errors} inference errors out of {i+1} inferences\")\n",
    "results = np.vstack(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a636b7b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T23:01:04.476421Z",
     "iopub.status.busy": "2025-08-10T23:01:04.475347Z",
     "iopub.status.idle": "2025-08-10T23:01:04.595411Z",
     "shell.execute_reply": "2025-08-10T23:01:04.594560Z"
    },
    "papermill": {
     "duration": 0.130956,
     "end_time": "2025-08-10T23:01:04.597200",
     "exception": false,
     "start_time": "2025-08-10T23:01:04.466244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "probs = [x[1] for x in results]\n",
    "# === 添加的改进代码 ===\n",
    "enhanced_probs = []\n",
    "for i, prob in enumerate(probs):\n",
    "    row = test.iloc[i]\n",
    "    examples = {\n",
    "        'positive_example_1': row['positive_example_1'],\n",
    "        'positive_example_2': row['positive_example_2'],\n",
    "        'negative_example_1': row['negative_example_1'], \n",
    "        'negative_example_2': row['negative_example_2']\n",
    "    }\n",
    "    enhanced_prob = two_stage_prediction(row['body'], row['rule'], examples, prob)\n",
    "    enhanced_probs.append(enhanced_prob)\n",
    "\n",
    "probs = enhanced_probs\n",
    "sub['rule_violation'] = probs\n",
    "sub.to_csv('submission_llama.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9d5428",
   "metadata": {
    "papermill": {
     "duration": 0.00758,
     "end_time": "2025-08-10T23:01:04.613660",
     "exception": false,
     "start_time": "2025-08-10T23:01:04.606080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. ENSEMBLE RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f58d2805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T23:01:04.629521Z",
     "iopub.status.busy": "2025-08-10T23:01:04.629240Z",
     "iopub.status.idle": "2025-08-10T23:01:04.762296Z",
     "shell.execute_reply": "2025-08-10T23:01:04.761025Z"
    },
    "papermill": {
     "duration": 0.14337,
     "end_time": "2025-08-10T23:01:04.764516",
     "exception": false,
     "start_time": "2025-08-10T23:01:04.621146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "q = pd.read_csv('submission_qwen.csv')\n",
    "l = pd.read_csv('submission_llama.csv')\n",
    "\n",
    "rq = q['rule_violation'].rank(method='average') / (len(q)+1)\n",
    "rl = l['rule_violation'].rank(method='average') / (len(l)+1)\n",
    "\n",
    "# blend = 0.5*rq + 0.5*rl   # or tune the rank-weights with a tiny grid using OOF\n",
    "# === 替换为自适应权重 ===\n",
    "test_df = pd.read_csv('/kaggle/input/jigsaw-agile-community-rules/test.csv')\n",
    "adaptive_predictions = []\n",
    "\n",
    "for i in range(len(q)):\n",
    "    row = test_df.iloc[i]\n",
    "    pos_examples = [row['positive_example_1'], row['positive_example_2']]\n",
    "    neg_examples = [row['negative_example_1'], row['negative_example_2']]\n",
    "    \n",
    "    features = extract_text_features(row['body'], row['rule'], pos_examples, neg_examples)\n",
    "    qwen_weight, llama_weight = get_adaptive_weights(features, row['subreddit'])\n",
    "    \n",
    "    qwen_pred = q.iloc[i]['rule_violation']\n",
    "    llama_pred = l.iloc[i]['rule_violation'] \n",
    "    \n",
    "    adaptive_pred = qwen_weight * qwen_pred + llama_weight * llama_pred\n",
    "    adaptive_predictions.append(adaptive_pred)\n",
    "\n",
    "q['rule_violation'] = adaptive_predictions\n",
    "# === 自适应权重结束 ===\n",
    "# q['rule_violation'] = blend\n",
    "q.to_csv('/kaggle/working/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "sourceId": 94635,
     "sourceType": "competition"
    },
    {
     "datasetId": 7940720,
     "sourceId": 12573785,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 252850661,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 252853424,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 570.165295,
   "end_time": "2025-08-10T23:01:08.819191",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-10T22:51:38.653896",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "066f03232c974b6cac3724cba696a7b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0b1f9f29d7ef4a3abc3c603204afaec7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0df7761e5e814e32a09d293ad3f30993": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "17a10d3ed7b648348c333bad55d296c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_660270b8e2474f0aa41d169bf3e36d28",
       "placeholder": "​",
       "style": "IPY_MODEL_ad0fa358f9984fdfa3e7c3f558872c23",
       "tabbable": null,
       "tooltip": null,
       "value": "Processed prompts: 100%"
      }
     },
     "243d5a8260a24048a975d04b535c1ed7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "46199ef6bee14819935b5f5335cbe386": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "476adb90cbc14fdaba4b5f2cd02eff4a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_bd3ee48ef82946e784ee9b535fb7bcfe",
        "IPY_MODEL_e343bff76d5a469c968c69bc6f0072ab",
        "IPY_MODEL_ea790e4b26704ea286640c314bf61175"
       ],
       "layout": "IPY_MODEL_46199ef6bee14819935b5f5335cbe386",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4e8aa8a9be864c5a93e54849370231f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "566748ba58534b83ad2ef3667069f359": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5fb58812ed2c44aa9b52b0adf4e0f938": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "61e09def368d432aa93b29744b1e53ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "653638dbbcd44e588facad260912aea1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "65edae037bd6483cb8ed22d674c2cefd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "660270b8e2474f0aa41d169bf3e36d28": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6fb0db9713a54a4bba5bc4a379dea0b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "70893d08054541888154da85d6378fbd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_17a10d3ed7b648348c333bad55d296c4",
        "IPY_MODEL_710308e6c95e484486dcd5a7e0c80985",
        "IPY_MODEL_eea953c5b8a3498ea7a0ff51113524e8"
       ],
       "layout": "IPY_MODEL_5fb58812ed2c44aa9b52b0adf4e0f938",
       "tabbable": null,
       "tooltip": null
      }
     },
     "710308e6c95e484486dcd5a7e0c80985": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_243d5a8260a24048a975d04b535c1ed7",
       "max": 10.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_65edae037bd6483cb8ed22d674c2cefd",
       "tabbable": null,
       "tooltip": null,
       "value": 10.0
      }
     },
     "732c269bfce542cc89eb18884b0f4914": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d0d8aadef00847d48469bf829943c09a",
       "max": 10.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_066f03232c974b6cac3724cba696a7b7",
       "tabbable": null,
       "tooltip": null,
       "value": 10.0
      }
     },
     "78ddef6949a547f5a59bd0ea2f2a3848": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4e8aa8a9be864c5a93e54849370231f5",
       "placeholder": "​",
       "style": "IPY_MODEL_d149ec0453074612813b2676e389ed9e",
       "tabbable": null,
       "tooltip": null,
       "value": " 10/10 [00:00&lt;00:00, 430.56it/s]"
      }
     },
     "8ae8796929664684bba668eb701ea5e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9d339e7cc0da4fb18e8eaf26ff4b12f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ad0fa358f9984fdfa3e7c3f558872c23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b7339d848d9f428fbf644531eeef82fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bd3ee48ef82946e784ee9b535fb7bcfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d2b48a4ddfe14417a088f736ebf22fc1",
       "placeholder": "​",
       "style": "IPY_MODEL_d42938bdc2b0426fac3e8df542da68cd",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "be24ed2260a54263bdfb2e4501264f85": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6fb0db9713a54a4bba5bc4a379dea0b6",
       "placeholder": "​",
       "style": "IPY_MODEL_8ae8796929664684bba668eb701ea5e4",
       "tabbable": null,
       "tooltip": null,
       "value": "Adding requests: 100%"
      }
     },
     "cd4a4ef7afb847ca91a3a1f966373582": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_be24ed2260a54263bdfb2e4501264f85",
        "IPY_MODEL_732c269bfce542cc89eb18884b0f4914",
        "IPY_MODEL_78ddef6949a547f5a59bd0ea2f2a3848"
       ],
       "layout": "IPY_MODEL_b7339d848d9f428fbf644531eeef82fc",
       "tabbable": null,
       "tooltip": null
      }
     },
     "d0d8aadef00847d48469bf829943c09a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d149ec0453074612813b2676e389ed9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d2b48a4ddfe14417a088f736ebf22fc1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d42938bdc2b0426fac3e8df542da68cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e343bff76d5a469c968c69bc6f0072ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_566748ba58534b83ad2ef3667069f359",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0b1f9f29d7ef4a3abc3c603204afaec7",
       "tabbable": null,
       "tooltip": null,
       "value": 4.0
      }
     },
     "ea790e4b26704ea286640c314bf61175": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_61e09def368d432aa93b29744b1e53ea",
       "placeholder": "​",
       "style": "IPY_MODEL_653638dbbcd44e588facad260912aea1",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:39&lt;00:00, 24.38s/it]\n"
      }
     },
     "eea953c5b8a3498ea7a0ff51113524e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0df7761e5e814e32a09d293ad3f30993",
       "placeholder": "​",
       "style": "IPY_MODEL_9d339e7cc0da4fb18e8eaf26ff4b12f0",
       "tabbable": null,
       "tooltip": null,
       "value": " 10/10 [00:01&lt;00:00,  4.18it/s, est. speed input: 2044.65 toks/s, output: 5.98 toks/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
